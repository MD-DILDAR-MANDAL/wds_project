{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Input\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(tf.config.list_physical_devices('GPU'))\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.set_visible_devices(gpus[0], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random.set_seed(42)\n",
    "\n",
    "base_path = 'dataset/'\n",
    "train_dir = os.path.join(base_path, 'train')\n",
    "\n",
    "train_csv = os.path.join(base_path, 'train', '_annotations.csv')\n",
    "valid_csv = os.path.join(base_path, 'valid', '_annotations.csv')\n",
    "test_csv = os.path.join(base_path, 'test', '_annotations.csv')\n",
    "                        \n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(filename, label, bbox):\n",
    "    try:\n",
    "        img = tf.io.read_file(filename)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, [360, 360])  # VGG16 input size\n",
    "        img = tf.cast(img, tf.float32)  / 255.0\n",
    "        #img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "        return img, label, bbox\n",
    "    except tf.errors.NotFoundError:\n",
    "        print(f\"File not found: {filename}\")\n",
    "        return None, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rifle' 'HandGun' 'ShotGun' 'SMG' 'Knife']\n",
      "Base Dir: dataset/train\n",
      "Labels shape: (5554,)\n",
      "Bounding boxes shape: (5554, 4)\n",
      "5\n",
      "['HandGun' 'Rifle' 'ShotGun' 'SMG' 'Knife']\n",
      "Base Dir: dataset/valid\n",
      "Labels shape: (824,)\n",
      "Bounding boxes shape: (824, 4)\n",
      "5\n",
      "['SMG' 'ShotGun' 'HandGun' 'Rifle' 'Knife']\n",
      "Base Dir: dataset/test\n",
      "Labels shape: (468,)\n",
      "Bounding boxes shape: (468, 4)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Function to load data from CSV and create a dataset\n",
    "def create_dataset(csv_file,base_dir, is_training=True):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    unique_labels = df['class'].unique()\n",
    "    print(unique_labels)\n",
    "    # Encode class labels\n",
    "    le = LabelEncoder()\n",
    "    df['class'] = le.fit_transform(df['class'])\n",
    "\n",
    "    filenames = df['filename'].apply(lambda x: os.path.join(base_dir, x)).values\n",
    "    labels = df['class'].values\n",
    "\n",
    "\n",
    "    bboxes = df[['xmin', 'ymin', 'xmax', 'ymax']].values\n",
    "    \n",
    "    #print(f\"filename:{filenames}\")\n",
    "    print(f\"Base Dir: {base_dir}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Bounding boxes shape: {bboxes.shape}\")\n",
    "    print(len(le.classes_))\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels, bboxes))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.filter(lambda x, y, z: x is not None)  # Filter out None values\n",
    " \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    \n",
    "    dataset = dataset.batch(32)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset, le\n",
    "\n",
    "# Create datasets one by one for train , valid , test\n",
    "train_dataset, label_encoder = create_dataset(train_csv, os.path.join(base_path, 'train'))\n",
    "valid_dataset, _ = create_dataset(valid_csv, os.path.join(base_path, 'valid'), is_training=False)\n",
    "test_dataset, _ = create_dataset(test_csv, os.path.join(base_path, 'test'), is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Batch images shape: (32, 360, 360, 3)\n",
      "Batch labels shape: (32,)\n",
      "Batch bounding boxes shape: (32, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>,    │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">65,664</span> │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ class_output        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bbox_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m360\u001b[0m, \u001b[38;5;34m360\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m,    │ \u001b[38;5;34m14,714,688\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m512\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │     \u001b[38;5;34m65,664\u001b[0m │ global_average_p… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ class_output        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m645\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bbox_output (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │        \u001b[38;5;34m516\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,781,513</span> (56.39 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,781,513\u001b[0m (56.39 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,825</span> (261.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,825\u001b[0m (261.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a custom VGG16 model\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(360, 360, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(360, 360, 3))\n",
    "\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "lent=5\n",
    "print(lent)\n",
    "class_output = Dense(5, activation='softmax', name='class_output')(x)\n",
    "bbox_output = Dense(4, name='bbox_output')(x)\n",
    "model = Model(inputs=inputs, outputs=[class_output, bbox_output])\n",
    "\n",
    "for batch in train_dataset.take(1):\n",
    "    images, labels, bboxes = batch\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Batch bounding boxes shape: {bboxes.shape}\")\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: [TensorShape([32, 360, 360, 3]), TensorShape([32]), TensorShape([32, 4])]\n",
      "Batch images shape: (32, 360, 360, 3)\n",
      "Batch labels shape: (32,)\n",
      "Batch bounding boxes shape: (32, 4)\n",
      "Batch images shape: (32, 360, 360, 3)\n",
      "Batch labels shape: (32,)\n",
      "Batch bounding boxes shape: (32, 4)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataset.take(1):\n",
    "    print(f\"Batch shapes: {[x.shape for x in batch]}\")\n",
    "\n",
    "# Check data shapes\n",
    "for batch in train_dataset.take(2):\n",
    "    images, labels, bboxes = batch\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Batch bounding boxes shape: {bboxes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes: [TensorShape([32, 360, 360, 3]), TensorShape([32]), TensorShape([32, 4])]\n",
      "Batch images shape: (32, 360, 360, 3)\n",
      "Batch labels shape: (32,)\n",
      "Batch bounding boxes shape: (32, 4)\n",
      "Batch images shape: (32, 360, 360, 3)\n",
      "Batch labels shape: (32,)\n",
      "Batch bounding boxes shape: (32, 4)\n"
     ]
    }
   ],
   "source": [
    "for batch1 in valid_dataset.take(1):\n",
    "    print(f\"Batch shapes: {[x.shape for x in batch]}\")\n",
    "\n",
    "# Check data shapes\n",
    "for batch1 in valid_dataset.take(2):\n",
    "    images, labels, bboxes = batch\n",
    "    print(f\"Batch images shape: {images.shape}\")\n",
    "    print(f\"Batch labels shape: {labels.shape}\")\n",
    "    print(f\"Batch bounding boxes shape: {bboxes.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss={'class_output': 'sparse_categorical_crossentropy', 'bbox_output': 'mse'},\n",
    "              metrics={'class_output': 'accuracy','bbox_output': 'mae'}\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(images, labels, bboxes):\n",
    "    with tf.GradientTape() as tape:\n",
    "        class_predictions, bbox_predictions = model(images, training=True)\n",
    "        class_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, class_predictions)\n",
    "        bbox_loss = tf.keras.losses.MeanSquaredError()(bboxes, bbox_predictions)  # Corrected\n",
    "        total_loss = tf.reduce_mean(tf.cast(class_loss, tf.float32)) + tf.reduce_mean(tf.cast(bbox_loss, tf.float32))\n",
    "        \n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss\n",
    "\n",
    "def validation_step(images, labels, bboxes):\n",
    "    predictions = model(images, training=False)\n",
    "    class_predictions, bbox_predictions = predictions\n",
    "    class_loss = tf.keras.losses.sparse_categorical_crossentropy(labels, class_predictions)\n",
    "    bbox_loss = tf.keras.losses.MeanSquaredError()(bboxes, bbox_predictions)\n",
    "    total_loss = tf.reduce_mean(class_loss) + tf.reduce_mean(bbox_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Load the latest checkpoint (if any)\n",
    "if checkpoint_manager.latest_checkpoint:\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    print(f\"Restored from {checkpoint_manager.latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Starting from scratch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Epoch 1 - Loss: 5164496.5000 - Validation Loss: 281473.7500\n",
      "Train Accuracy: 0.2803 - Validation Accuracy: 0.3714\n",
      "Time for epoch 1: 13640.68 seconds\n",
      "\n",
      "Checkpoint saved for epoch 1\n",
      "\n",
      "Epoch 2/10\n",
      "Epoch 2 - Loss: 1403893.5000 - Validation Loss: 192568.2344\n",
      "Train Accuracy: 0.2946 - Validation Accuracy: 0.2961\n",
      "Time for epoch 2: 15329.94 seconds\n",
      "\n",
      "Checkpoint saved for epoch 2\n",
      "\n",
      "Epoch 3/10\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "num_epochs = 10  # Set your number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    epoch_loss = 0\n",
    "    train_accuracy.reset_state()\n",
    "\n",
    "    # Training\n",
    "    for batch in train_dataset:\n",
    "        images, labels, bboxes = batch\n",
    "        batch_loss = train_step(images, labels, bboxes)\n",
    "        epoch_loss += batch_loss\n",
    "        train_accuracy.update_state(labels, model(images, training=True)[0])\n",
    "\n",
    "    # Validation\n",
    "    val_loss = 0\n",
    "    val_accuracy.reset_state()\n",
    "    for batch1 in valid_dataset:\n",
    "        images, labels, bboxes = batch1\n",
    "        batch_val_loss = validation_step(images, labels, bboxes)\n",
    "        val_loss += batch_val_loss\n",
    "        val_accuracy.update_state(labels, model(images, training=False)[0])\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    #print(f'Epoch {epoch + 1} - Loss: {epoch_loss.numpy()} - Validation Loss: {val_loss.numpy()}')\n",
    "    print(f'Epoch {epoch + 1} - Loss: {epoch_loss.numpy():.4f} - Validation Loss: {val_loss.numpy():.4f}')\n",
    "    print(f'Train Accuracy: {train_accuracy.result().numpy():.4f} - Validation Accuracy: {val_accuracy.result().numpy():.4f}')\n",
    "    print(f'Time for epoch {epoch + 1}: {elapsed_time:.2f} seconds\\n')\n",
    "\n",
    "    # Save the model's weights after each epoch\n",
    "    checkpoint_manager.save()\n",
    "    print(f\"Checkpoint saved for epoch {epoch + 1}\\n\")\n",
    "    \n",
    "    # After the process finishes+\n",
    "    duration = 1000  # milliseconds\n",
    "    freq = 440  # Hz\n",
    "    winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "test_loss = 0\n",
    "for batch in test_dataset:\n",
    "    images, labels, bboxes = batch\n",
    "    batch_test_loss = validation_step(images, labels, bboxes)  # You can reuse validation_step here\n",
    "    test_loss += batch_test_loss\n",
    "\n",
    "print(f'Test Loss: {test_loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataset.take(1):  # Take one batch of images\n",
    "    images, labels, bboxes = batch\n",
    "    pred_class, pred_bboxes = model.predict(images)\n",
    "    \n",
    "    # Display one example\n",
    "    plt.imshow(images[0])\n",
    "    plt.title(f'Predicted class: {pred_class[0]}, True class: {labels[0]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification accuracy\n",
    "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "for batch in test_dataset:\n",
    "    images, labels, bboxes = batch\n",
    "    pred_class, pred_bboxes = model(images, training=False)\n",
    "    accuracy.update_state(labels, pred_class)\n",
    "\n",
    "print(f'Test Accuracy: {accuracy.result().numpy()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#             train_dataset,\n",
    "#             validation_data=valid_dataset,\n",
    "#             epochs=10,  # Adjust as needed\n",
    "#             verbose=1\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
